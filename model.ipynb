{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    return nd.mean(nd.argmax(output, axis=1) == labels).asscalar()\n",
    "\n",
    "def evaluate(net, data):\n",
    "    loss, acc, n = 0., 0., 0.\n",
    "    steps = len(data_iter_val)\n",
    "    for data, label in data:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "        loss += nd.mean(softmax_cross_entropy(output, label)).asscalar()\n",
    "    return loss/steps, acc/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('features.h5', 'r') as f:\n",
    "    features_vgg = np.array(f['vgg'])\n",
    "    features_resnet = np.array(f['resnet'])\n",
    "    features_densenet = np.array(f['densenet'])\n",
    "    features_inception = np.array(f['inception'])\n",
    "    labels = np.array(f['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_resnet = features_resnet.reshape(features_resnet.shape[:2])\n",
    "features_inception = features_inception.reshape(features_inception.shape[:2])\n",
    "\n",
    "features = np.concatenate([features_resnet, features_densenet, features_inception], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(features, labels)\n",
    "\n",
    "dataset_train = gluon.data.ArrayDataset(nd.array(X_train), nd.array(y_train))\n",
    "dataset_val = gluon.data.ArrayDataset(nd.array(X_val), nd.array(y_val))\n",
    "\n",
    "batch_size = 128\n",
    "data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "data_iter_val = gluon.data.DataLoader(dataset_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dense(None -> 256, Activation(relu))\n",
       "  (1): Dropout(p = 0.5)\n",
       "  (2): Dense(None -> 120, linear)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(256, activation='relu'))\n",
    "    net.add(nn.Dropout(0.5))\n",
    "    net.add(nn.Dense(120))\n",
    "\n",
    "net.initialize(ctx=ctx)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': 1e-4, 'wd': 1e-5})\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss: 4.6348, acc: 4.92%, val_loss 3.9883, val_acc 23.46%\n",
      "Epoch 2. loss: 3.5632, acc: 23.09%, val_loss 2.6124, val_acc 61.97%\n",
      "Epoch 3. loss: 2.4211, acc: 46.51%, val_loss 1.5162, val_acc 78.45%\n",
      "Epoch 4. loss: 1.6728, acc: 60.84%, val_loss 1.0034, val_acc 83.65%\n",
      "Epoch 5. loss: 1.2647, acc: 69.35%, val_loss 0.7494, val_acc 85.72%\n",
      "Epoch 6. loss: 1.0422, acc: 73.79%, val_loss 0.6181, val_acc 87.05%\n",
      "Epoch 7. loss: 0.8917, acc: 77.17%, val_loss 0.5362, val_acc 88.38%\n",
      "Epoch 8. loss: 0.7709, acc: 80.29%, val_loss 0.4820, val_acc 88.46%\n",
      "Epoch 9. loss: 0.6899, acc: 81.59%, val_loss 0.4492, val_acc 88.93%\n",
      "Epoch 10. loss: 0.6352, acc: 82.74%, val_loss 0.4185, val_acc 88.97%\n",
      "Epoch 11. loss: 0.5687, acc: 84.11%, val_loss 0.4027, val_acc 89.40%\n",
      "Epoch 12. loss: 0.5438, acc: 84.98%, val_loss 0.3859, val_acc 89.40%\n",
      "Epoch 13. loss: 0.4938, acc: 86.28%, val_loss 0.3721, val_acc 89.40%\n",
      "Epoch 14. loss: 0.4673, acc: 86.43%, val_loss 0.3647, val_acc 89.40%\n",
      "Epoch 15. loss: 0.4432, acc: 87.38%, val_loss 0.3524, val_acc 89.90%\n",
      "Epoch 16. loss: 0.4159, acc: 88.14%, val_loss 0.3446, val_acc 90.18%\n",
      "Epoch 17. loss: 0.3957, acc: 88.81%, val_loss 0.3417, val_acc 89.98%\n",
      "Epoch 18. loss: 0.3770, acc: 88.50%, val_loss 0.3353, val_acc 90.06%\n",
      "Epoch 19. loss: 0.3563, acc: 89.62%, val_loss 0.3296, val_acc 90.30%\n",
      "Epoch 20. loss: 0.3537, acc: 89.69%, val_loss 0.3285, val_acc 89.98%\n",
      "Epoch 21. loss: 0.3345, acc: 90.22%, val_loss 0.3244, val_acc 90.22%\n",
      "Epoch 22. loss: 0.3184, acc: 90.63%, val_loss 0.3199, val_acc 90.65%\n",
      "Epoch 23. loss: 0.3118, acc: 90.93%, val_loss 0.3191, val_acc 90.26%\n",
      "Epoch 24. loss: 0.2982, acc: 91.29%, val_loss 0.3147, val_acc 90.69%\n",
      "Epoch 25. loss: 0.2815, acc: 91.94%, val_loss 0.3133, val_acc 90.73%\n",
      "Epoch 26. loss: 0.2716, acc: 92.15%, val_loss 0.3129, val_acc 90.33%\n",
      "Epoch 27. loss: 0.2619, acc: 92.42%, val_loss 0.3129, val_acc 90.53%\n",
      "Epoch 28. loss: 0.2557, acc: 92.71%, val_loss 0.3091, val_acc 90.30%\n",
      "Epoch 29. loss: 0.2525, acc: 92.50%, val_loss 0.3068, val_acc 90.85%\n",
      "Epoch 30. loss: 0.2439, acc: 92.76%, val_loss 0.3050, val_acc 90.57%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    steps = len(data_iter_train)\n",
    "    for data, label in data_iter_train:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += accuracy(output, label)\n",
    "    \n",
    "    val_loss, val_acc = evaluate(net, data_iter_val)\n",
    "    print(\"Epoch %d. loss: %.4f, acc: %.2f%%, val_loss %.4f, val_acc %.2f%%\" % (\n",
    "        epoch+1, train_loss/steps, train_acc/steps*100, val_loss, val_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('features_test.h5', 'r') as f:\n",
    "    features_vgg_test = np.array(f['vgg'])\n",
    "    features_resnet_test = np.array(f['resnet'])\n",
    "    features_densenet_test = np.array(f['densenet'])\n",
    "    features_inception_test = np.array(f['inception'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_resnet_test = features_resnet_test.reshape(features_resnet_test.shape[:2])\n",
    "features_inception_test = features_inception_test.reshape(features_inception_test.shape[:2])\n",
    "\n",
    "features_test = np.concatenate([features_resnet_test, features_densenet_test, features_inception_test], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nd.softmax(net(nd.array(features_test).as_in_context(ctx))).asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "for i, c in enumerate(df.columns[1:]):\n",
    "    df[c] = output[:,i]\n",
    "\n",
    "df.to_csv('pred.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
