{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    return nd.mean(nd.argmax(output, axis=1) == labels).asscalar()\n",
    "\n",
    "def evaluate(net, data_iter):\n",
    "    loss, acc, n = 0., 0., 0.\n",
    "    steps = len(data_iter)\n",
    "    for data, label in data_iter:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "        loss += nd.mean(softmax_cross_entropy(output, label)).asscalar()\n",
    "    return loss/steps, acc/steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入之前计算的特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('features.h5', 'r') as f:\n",
    "    features_vgg = np.array(f['vgg'])\n",
    "    features_resnet = np.array(f['resnet'])\n",
    "    features_densenet = np.array(f['densenet'])\n",
    "    features_inception = np.array(f['inception'])\n",
    "    labels = np.array(f['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_resnet = features_resnet.reshape(features_resnet.shape[:2])\n",
    "features_inception = features_inception.reshape(features_inception.shape[:2])\n",
    "\n",
    "features = np.concatenate([features_resnet, features_densenet, features_inception], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用 ArrayDataset 和 DataLoader 构建迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2)\n",
    "\n",
    "dataset_train = gluon.data.ArrayDataset(nd.array(X_train), nd.array(y_train))\n",
    "dataset_val = gluon.data.ArrayDataset(nd.array(X_val), nd.array(y_val))\n",
    "\n",
    "batch_size = 128\n",
    "data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "data_iter_val = gluon.data.DataLoader(dataset_val, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dense(None -> 256, Activation(relu))\n",
       "  (1): Dropout(p = 0.5)\n",
       "  (2): Dense(None -> 120, linear)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(256, activation='relu'))\n",
    "    net.add(nn.Dropout(0.5))\n",
    "    net.add(nn.Dense(120))\n",
    "\n",
    "net.initialize(ctx=ctx)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': 1e-4, 'wd': 1e-5})\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练50代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss: 4.5991, acc: 5.27%, val_loss 3.8873, val_acc 26.60%\n",
      "Epoch 2. loss: 3.4482, acc: 24.89%, val_loss 2.3933, val_acc 66.30%\n",
      "Epoch 3. loss: 2.2899, acc: 49.16%, val_loss 1.3686, val_acc 80.34%\n",
      "Epoch 4. loss: 1.5658, acc: 63.39%, val_loss 0.8952, val_acc 84.36%\n",
      "Epoch 5. loss: 1.1688, acc: 71.79%, val_loss 0.6645, val_acc 86.07%\n",
      "Epoch 6. loss: 0.9619, acc: 75.25%, val_loss 0.5501, val_acc 87.53%\n",
      "Epoch 7. loss: 0.8236, acc: 77.80%, val_loss 0.4851, val_acc 87.97%\n",
      "Epoch 8. loss: 0.7274, acc: 80.09%, val_loss 0.4418, val_acc 88.31%\n",
      "Epoch 9. loss: 0.6635, acc: 82.39%, val_loss 0.4173, val_acc 88.12%\n",
      "Epoch 10. loss: 0.6234, acc: 83.12%, val_loss 0.3868, val_acc 89.00%\n",
      "Epoch 11. loss: 0.5592, acc: 84.63%, val_loss 0.3676, val_acc 89.00%\n",
      "Epoch 12. loss: 0.5104, acc: 85.71%, val_loss 0.3538, val_acc 89.44%\n",
      "Epoch 13. loss: 0.4838, acc: 86.50%, val_loss 0.3492, val_acc 89.20%\n",
      "Epoch 14. loss: 0.4568, acc: 87.08%, val_loss 0.3333, val_acc 89.49%\n",
      "Epoch 15. loss: 0.4347, acc: 87.71%, val_loss 0.3319, val_acc 89.54%\n",
      "Epoch 16. loss: 0.4033, acc: 88.71%, val_loss 0.3203, val_acc 90.22%\n",
      "Epoch 17. loss: 0.3892, acc: 88.25%, val_loss 0.3134, val_acc 89.88%\n",
      "Epoch 18. loss: 0.3709, acc: 89.42%, val_loss 0.3148, val_acc 89.98%\n",
      "Epoch 19. loss: 0.3533, acc: 89.75%, val_loss 0.3072, val_acc 90.32%\n",
      "Epoch 20. loss: 0.3464, acc: 89.74%, val_loss 0.3062, val_acc 89.88%\n",
      "Epoch 21. loss: 0.3315, acc: 90.31%, val_loss 0.2986, val_acc 90.57%\n",
      "Epoch 22. loss: 0.3187, acc: 90.84%, val_loss 0.3009, val_acc 90.32%\n",
      "Epoch 23. loss: 0.2961, acc: 91.26%, val_loss 0.2936, val_acc 90.71%\n",
      "Epoch 24. loss: 0.2913, acc: 91.40%, val_loss 0.2989, val_acc 90.27%\n",
      "Epoch 25. loss: 0.2840, acc: 91.80%, val_loss 0.2946, val_acc 90.42%\n",
      "Epoch 26. loss: 0.2686, acc: 92.59%, val_loss 0.2909, val_acc 90.37%\n",
      "Epoch 27. loss: 0.2609, acc: 92.65%, val_loss 0.2891, val_acc 90.32%\n",
      "Epoch 28. loss: 0.2564, acc: 92.67%, val_loss 0.2842, val_acc 90.71%\n",
      "Epoch 29. loss: 0.2461, acc: 92.24%, val_loss 0.2849, val_acc 90.47%\n",
      "Epoch 30. loss: 0.2320, acc: 93.28%, val_loss 0.2845, val_acc 90.61%\n",
      "Epoch 31. loss: 0.2289, acc: 93.33%, val_loss 0.2816, val_acc 90.62%\n",
      "Epoch 32. loss: 0.2181, acc: 93.56%, val_loss 0.2843, val_acc 90.27%\n",
      "Epoch 33. loss: 0.2132, acc: 93.90%, val_loss 0.2865, val_acc 90.32%\n",
      "Epoch 34. loss: 0.2000, acc: 94.37%, val_loss 0.2854, val_acc 90.32%\n",
      "Epoch 35. loss: 0.1963, acc: 94.31%, val_loss 0.2813, val_acc 90.42%\n",
      "Epoch 36. loss: 0.1919, acc: 94.37%, val_loss 0.2777, val_acc 89.93%\n",
      "Epoch 37. loss: 0.1762, acc: 95.29%, val_loss 0.2856, val_acc 90.17%\n",
      "Epoch 38. loss: 0.1826, acc: 94.94%, val_loss 0.2816, val_acc 90.47%\n",
      "Epoch 39. loss: 0.1718, acc: 95.23%, val_loss 0.2761, val_acc 90.86%\n",
      "Epoch 40. loss: 0.1691, acc: 95.23%, val_loss 0.2823, val_acc 90.32%\n",
      "Epoch 41. loss: 0.1649, acc: 95.32%, val_loss 0.2789, val_acc 90.96%\n",
      "Epoch 42. loss: 0.1571, acc: 95.49%, val_loss 0.2803, val_acc 90.42%\n",
      "Epoch 43. loss: 0.1511, acc: 95.90%, val_loss 0.2769, val_acc 90.37%\n",
      "Epoch 44. loss: 0.1477, acc: 95.92%, val_loss 0.2778, val_acc 90.66%\n",
      "Epoch 45. loss: 0.1519, acc: 95.89%, val_loss 0.2771, val_acc 90.37%\n",
      "Epoch 46. loss: 0.1383, acc: 96.23%, val_loss 0.2755, val_acc 90.61%\n",
      "Epoch 47. loss: 0.1472, acc: 95.81%, val_loss 0.2749, val_acc 90.71%\n",
      "Epoch 48. loss: 0.1343, acc: 96.32%, val_loss 0.2796, val_acc 90.42%\n",
      "Epoch 49. loss: 0.1272, acc: 96.52%, val_loss 0.2794, val_acc 90.47%\n",
      "Epoch 50. loss: 0.1249, acc: 96.48%, val_loss 0.2757, val_acc 90.56%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    steps = len(data_iter_train)\n",
    "    for data, label in data_iter_train:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += accuracy(output, label)\n",
    "    \n",
    "    val_loss, val_acc = evaluate(net, data_iter_val)\n",
    "    print(\"Epoch %d. loss: %.4f, acc: %.2f%%, val_loss %.4f, val_acc %.2f%%\" % (\n",
    "        epoch+1, train_loss/steps, train_acc/steps*100, val_loss, val_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入测试集特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('features_test.h5', 'r') as f:\n",
    "    features_vgg_test = np.array(f['vgg'])\n",
    "    features_resnet_test = np.array(f['resnet'])\n",
    "    features_densenet_test = np.array(f['densenet'])\n",
    "    features_inception_test = np.array(f['inception'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_resnet_test = features_resnet_test.reshape(features_resnet_test.shape[:2])\n",
    "features_inception_test = features_inception_test.reshape(features_inception_test.shape[:2])\n",
    "\n",
    "features_test = np.concatenate([features_resnet_test, features_densenet_test, features_inception_test], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用模型进行预测并输出到 pred.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nd.softmax(net(nd.array(features_test).as_in_context(ctx))).asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "for i, c in enumerate(df.columns[1:]):\n",
    "    df[c] = output[:,i]\n",
    "\n",
    "df.to_csv('pred.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
